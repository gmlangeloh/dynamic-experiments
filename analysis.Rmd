---
title: "Evaluating dynamic Gröbner Basis algorithms"
author: "Gabriel Mattos Langeloh"
date: "10/10/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)

dyn_results <- read.table("basic_data2.out", header=T)
```

In this document, I will analyze some results of dynamic Gröbner Basis algorithms. The following algorithms and instances were used to generate the data.

```{r}
"Instances"
unique(dyn_results$instance)
"Algorithms"
unique(dyn_results$algorithm)
```

We take the mean over all repetitions for each pair (algorithm, instance).

```{r}
by_instance <- dyn_results %>% group_by(instance)
by_algorithm <- dyn_results %>% group_by(algorithm)
by_inst_alg <- dyn_results %>% 
  group_by(instance, algorithm) %>% 
  summarize_all(mean) %>%
  select(-rep)
print(by_inst_alg)
```

First, we want to visualize the running time of the algorithms per instance, comparatively, and to find the algorithm that runs the fastest for each instance.

```{r}
ggplot(by_inst_alg, aes(x=instance, y=time, fill=algorithm)) + 
  geom_col(position='dodge') + 
  coord_flip()
#Fastest algorithm
by_inst_alg %>% 
  filter(rank(time) == 1) %>% 
  select(instance, algorithm)
#Slowest algorithm
by_inst_alg %>% 
  filter(rank(desc(time)) == 1) %>% 
  select(instance, algorithm)
```

We observe that static is the fastest algorithm on most instances, and caboara-perry is slowest on most. These instances are pretty biased towards static, though, as they are small enough so that the dynamic overhead makes a big difference.

Now, we compare the sizes of the output bases, in number of polynomials.

```{r}
ggplot(by_inst_alg, aes(x=instance, y=polynomials, fill=algorithm)) + 
  geom_col(position='dodge') + 
  coord_flip()
#Smallest basis
by_inst_alg %>% 
  filter(rank(time) == 1) %>% 
  select(instance, algorithm)
#Largest basis
by_inst_alg %>% 
  filter(rank(desc(time)) == 1) %>% 
  select(instance, algorithm)

```

Here, the dynamic algorithms (perturbation and caboara-perry) get better results than static for larger instances, such as cyclicn6 and cyclicnh6. All algorithms tie or are close to tying for the katsura family. It can be shown that the affine Katsura instance with parameter $n$ has a Gröbner Basis with $n$ polynomials. All algorithms are far from this lower bound, which means the dynamic algorithms should be improved to deal with this kind of situation better.

We should also check what happens to the degrees.

```{r}
ggplot(by_inst_alg, aes(x=instance, y=degree, fill=algorithm)) +
  geom_col(position='dodge') + 
  coord_flip()
#Smallest degree
by_inst_alg %>% 
  filter(rank(time) == 1) %>% 
  select(instance, algorithm)
#Largest degree
by_inst_alg %>% 
  filter(rank(desc(time)) == 1) %>% 
  select(instance, algorithm)
```

Algorithms tie in terms of degree for most Katsuras. For the cyclics, perturbation seems to perform well, specially for the larger ones.

Quick idea: can we show that getting smaller bases rises the degree? (the graphic below looks awful, but I think it shows that the answer is yes for some instances, no to others).

```{r}
ggplot(by_inst_alg, aes(x=polynomials, y=degree, color=instance)) +
  geom_line()
```

Check correlation between number of S-reductions and time.

```{r}
ggplot(by_inst_alg, aes(x=sreductions, y=time, color=algorithm)) +
  geom_point() + 
  scale_x_log10() + 
  scale_y_log10()
```

There is clearly a positive correlation (graphically) and computing it we get
`r cor(by_inst_alg$sreductions, by_inst_alg$time)`.

We should also test:
- polynomials and time: `r cor(by_inst_alg$polynomials, by_inst_alg$time)`
- monomials and time: `r cor(by_inst_alg$monomials, by_inst_alg$time)`
- degree and time: `r cor(by_inst_alg$degree, by_inst_alg$time)`

The first two are aroung $0.5$, degree is $0.81$. Graphing degree, we get:

```{r}
ggplot(by_inst_alg, aes(x=degree, y=time, color=algorithm)) +
  geom_point() + 
  scale_x_log10() + 
  scale_y_log10()
```