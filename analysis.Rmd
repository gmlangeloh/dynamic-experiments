---
title: "Evaluating dynamic Gröbner Basis algorithms"
author: "Gabriel Mattos Langeloh"
date: "10/10/2019"
output: pdf_document
bibliography: biblio.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, eval = TRUE)
library(tidyverse)

dyn_results <- read.table("tiamat0-results5.out", header=T)
```

# Introduction

The goal of this document is to do some exploratory analysis on dynamic Gröbner Basis algorithms. *Gröbner bases* 
(GB) are a fundamental tool in computer algebra to solve multivariate polynomial (non-linear) systems, among other
applications. Traditional *static* Gröbner Basis algorithms receive an ordering (a vector) as part of the
input. The performance of the algorithm, as well as the size of the output itself strongly depends on this input
ordering. *Dynamic* Gröbner Basis algorithms were proposed to avoid having to choose an ordering a priori,
computing one during the execution of the algorithm itself, hoping that this would lead to at least some
of the following:

- shorter computation times
- fewer polynomials in the output
- sparser polynomials in the output
- polynomials of lower degree in the output

These are the main values that are computed in the experiments that follow. Four algorithms were used: 

- the static algorithm (with a fixed input ordering, commonly used in the literature)
- the caboara-perry algorithm (proposed in [@Caboara2014], uses linear programming)
- the perturbation algorithm (new, looks for better orderings by applying perturbations to the current one)
- the random algorithm (new, does a random walk in the space of orderings)

Due to particular traits of these algorithms, the perturbation and random algorithms only look for new orderings
every 10 iterations of the Gröbner basis computation. It would also be interesting to find a better period
more rigorously, but we do not do that here.

# Experimental setup and environment

All algorithms were implemented in the Sage computer algebra system (version 8.8, Python version 3.7.3) 
and share all basic functionality from the underlying algorithm used to compute Gröbner Bases.
Our implementation is based on that of [@Caboara2014].
Experiments were run on an Intel(R) Core(TM) i7 CPU 930 @ 2.80GHz machine with 12GB of RAM.

All `r length(unique(dyn_results$instance))` instances were extracted from 
[Christian Eder's benchmark library](https://github.com/ederc/singular-benchmarks) for 
Gröbner Bases. Each algorithm was run on every instance 30 times, the presented results corresponding to the
average of these runs.

```{r, eval=FALSE, include=FALSE}
"Instances"
unique(dyn_results$instance)
"Algorithms"
unique(dyn_results$algorithm)
```

```{r, include=FALSE}
by_instance <- dyn_results %>% group_by(instance)
by_algorithm <- dyn_results %>% group_by(algorithm)
by_inst_alg <- dyn_results %>% 
  group_by(instance, algorithm, reducer) %>% 
  summarize_all(mean) %>%
  select(-rep)
buchberger <- by_inst_alg %>% filter(reducer == 'classical')
f4 <- by_inst_alg %>% filter(reducer == 'F4')
```

```{r}
by_inst_alg
```

# Partial results from some previous works

The following table shows the results reported in [@Caboara2014]. Timings are not reported in the paper. 
We cannot reproduce these results, even using the code supplied in the original paper. 

```{r}
caboara_perry <- read.table("caboara-perry2014.out", header = TRUE)
knitr::kable(
  caboara_perry,
  caption='Results from Caboara and Perry, 2014'
)
```

For completeness, we also show the results from [@Perry2017], that uses a slightly modified version of the 
caboara-perry algorithm and a simple implementation in C++.

```{r}
perry <- read.table("perry2017.out", header = TRUE)
knitr::kable(
  perry,
  caption="Results from Perry, 2017"
)
knitr::kable(
  buchberger %>% filter(algorithm == "caboara-perry", grepl("cyclicn", instance)) %>%
    select(instance, polynomials, monomials, time),
  caption="Current caboara-Perry results for cyclic instances"
)
```

# Exploratory analysis

First, we want to visualize the running time of the algorithms per instance, comparatively, and to find the algorithm that runs the fastest for each instance.

```{r}
ggplot(buchberger, aes(x=instance, y=time, fill=algorithm)) + 
  geom_col(position='dodge') + 
  coord_flip()

knitr::kable(
  buchberger %>% 
    filter(rank(time) == 1) %>% 
    select(instance, algorithm),
  caption = "Fastest algorithm per instance"
)

knitr::kable(
  buchberger %>% 
    filter(rank(desc(time)) == 1) %>% 
    select(instance, algorithm),
  caption = "Slowest algorithm per instance"
)
```

We observe that static is the fastest algorithm on most instances, and caboara-perry is slowest on most. These instances are pretty biased towards static, though, as they are small enough so that the dynamic overhead makes a big difference.

Now, we compare the sizes of the output bases, in number of polynomials.

```{r}
ggplot(buchberger, aes(x=instance, y=polynomials, fill=algorithm)) + 
  geom_col(position='dodge') + 
  coord_flip()
#Smallest basis
knitr::kable(
  buchberger %>% 
   filter(rank(time) == 1) %>% 
    select(instance, algorithm),
  caption="Algorithm with smallest basis for each instance"
)
#Largest basis
knitr::kable(
  buchberger %>% 
    filter(rank(desc(time)) == 1) %>% 
    select(instance, algorithm),
  caption="Algorithm with largest basis for each instance"
)
```

Here, the dynamic algorithms (perturbation and caboara-perry) get better results than static for larger instances, such as cyclicn6 and cyclicnh6. All algorithms tie or are close to tying for the katsura family. It can be shown that the affine Katsura instance with parameter $n$ has a Gröbner Basis with $n$ polynomials. All algorithms are far from this lower bound, which means the dynamic algorithms should be improved to deal with this kind of situation better.

We should also check what happens to the degrees.

```{r}
ggplot(buchberger, aes(x=instance, y=degree, fill=algorithm)) +
  geom_col(position='dodge') + 
  coord_flip()
#Smallest degree
knitr::kable(
  buchberger %>% 
    filter(rank(time) == 1) %>% 
    select(instance, algorithm),
  caption="Algorithm with smallest degree for each instance"
)
#Largest degree
knitr::kable(
  buchberger %>% 
    filter(rank(desc(time)) == 1) %>% 
    select(instance, algorithm),
  caption="Algorithm with largest degree for each instance"
)
```

Algorithms tie in terms of degree for most Katsuras. For the cyclics, perturbation seems to perform well, specially for the larger ones.

Quick idea: can we show that getting smaller bases rises the degree? (the graphic below looks awful, but I think it shows that the answer is yes for some instances, no to others).

```{r}
ggplot(buchberger, aes(x=polynomials, y=degree, color=instance)) +
  geom_line()
```

Check correlation between number of S-reductions and time.

```{r}
ggplot(buchberger, aes(x=sreductions, y=time, color=algorithm)) +
  geom_point() + 
  scale_x_log10() + 
  scale_y_log10()
```

There is clearly a positive correlation (graphically) and computing it we get
`r cor(buchberger$sreductions, buchberger$time)`.

We should also test:

- polynomials and time: `r cor(buchberger$polynomials, buchberger$time)`
- monomials and time: `r cor(buchberger$monomials, buchberger$time)`
- degree and time: `r cor(buchberger$degree, buchberger$time)`

The first two are aroung $0.5$, degree is $0.81$. Graphing degree, we get:

```{r}
ggplot(buchberger, aes(x=degree, y=time, color=algorithm)) +
  geom_point() + 
  scale_x_log10() + 
  scale_y_log10()
```

I should also measure fraction of time taken managing the queue, unrestricted vs restricted algorithms.

```{r}
ggplot(buchberger, aes(x=instance, y=queue / time, fill=algorithm)) +
  geom_col(position='dodge') + 
  coord_flip()
```

# Classical reduction vs F4

```{r}
mean(buchberger$time, na.rm=TRUE)
mean(f4$time, na.rm=TRUE)
summary(buchberger)
summary(f4)
```

```{r}
gmean <- function(x, na.rm=FALSE) {
  exp(mean(log(x), na.rm=na.rm))
}
gmean(c(2.0, 0.5))
gmean(c(1.0, 1.0))
```

```{r}
cp <- by_inst_alg %>% filter(algorithm == 'caboara-perry')
static <- by_inst_alg %>% filter(algorithm == 'static')
perturb <- by_inst_alg %>% filter(algorithm == 'perturbation')

ggplot(cp, aes(x=instance, y=1 + time, fill=reducer)) + 
  geom_col(position='dodge') + 
  coord_flip() + 
  scale_y_log10()

ggplot(static, aes(x=instance, y=1 + time, fill=reducer)) + 
  geom_col(position='dodge') + 
  coord_flip() + 
  scale_y_log10()

ggplot(perturb, aes(x=instance, y=1 + time, fill=reducer)) + 
  geom_col(position='dodge') + 
  coord_flip() + 
  scale_y_log10()

#Compare classical and F4 running times
cp %>% group_by(reducer) %>% summarize(mean=gmean(time, na.rm=TRUE))
static %>% group_by(reducer) %>% summarize(mean=gmean(time, na.rm=TRUE))
perturb %>% group_by(reducer) %>% summarize(mean=gmean(time, na.rm=TRUE))

cp %>%group_by(reducer) %>% mutate(time=replace_na(time, 3600)) %>% summarize(mean=gmean(time))
static %>%group_by(reducer) %>% mutate(time=replace_na(time, 3600)) %>% summarize(mean=gmean(time))
perturb %>%group_by(reducer) %>% mutate(time=replace_na(time, 3600)) %>% summarize(mean=gmean(time))

#Remove "outliers" and compare again
cp %>% 
  group_by(reducer) %>% 
  filter(time < quantile(cp$time, 0.95, na.rm=TRUE)) %>% 
  summarize(mean=gmean(time, na.rm=TRUE))
static %>% 
  group_by(reducer) %>% 
  filter(time < quantile(static$time, 0.95, na.rm=TRUE)) %>% 
  summarize(mean=gmean(time, na.rm=TRUE))
perturb %>% 
  group_by(reducer) %>% 
  filter(time < quantile(perturb$time, 0.95, na.rm=TRUE)) %>% 
  summarize(mean=gmean(time, na.rm=TRUE))
#Now F4 doesn't look more efficient at all! Check which ones are those outlier instances
cp %>% 
  group_by(reducer) %>% 
  filter(time >= quantile(cp$time, 0.95, na.rm=TRUE))
static %>% 
  group_by(reducer) %>% 
  filter(time >= quantile(static$time, 0.95, na.rm=TRUE))
perturb %>% 
  group_by(reducer) %>% 
  filter(time >= quantile(perturb$time, 0.95, na.rm=TRUE))
```

```{r}
by_inst_alg %>% 
  group_by(algorithm, reducer) %>% 
  summarize(time = mean(time, na.rm=TRUE), polynomials = mean(polynomials, na.rm=TRUE)) %>% 
  select(algorithm, reducer, time, polynomials)
```

#Generating some tables for the paper

```{r}
library(xtable)
paper_format <- function (table, alg, caption) {
  t <-table %>%
    filter(algorithm == alg) %>%
    ungroup() %>%
    select(-algorithm,-reducer,-queue,-heuristic,-reduction,-zeroreductions) %>%
    mutate(
      polynomials = as.integer(polynomials),
      monomials = as.integer(monomials),
      degree = as.integer(degree),
      sreductions = as.integer(sreductions)
    ) %>%
    rename(
      "$t$" = time,
      "$O$" = dynamic,
      "$|G|$" = polynomials,
      "$|\\Supp(G)|$" = monomials,
      "$\\deg$" = degree,
      "sred" = sreductions
    )
  print(xtable(t,caption=caption), 
        include.rownames=F, 
        sanitize.text.function=function(x){x},
        NA.string = "NA")
}
```

```{r}
algorithms <- unique(by_inst_alg$algorithm)
for (alg in algorithms) {
  cap1 <- paste("Experimental results for ", alg, " using Buchberger reducer")
  cap2 <- paste("Experimental results for ", alg, " using F4 reducer")
  paper_format(buchberger, alg, cap1)
  paper_format(f4, alg, cap2)
}
```

## Comparing running times, basis sizes, ...

This is very interesting data, I should probably make a nice table showing THIS.

```{r}
cp %>% group_by(reducer) %>% summarize(mean=gmean(time, na.rm=TRUE))
static %>% group_by(reducer) %>% summarize(mean=gmean(time, na.rm=TRUE))
perturb %>% group_by(reducer) %>% summarize(mean=gmean(time, na.rm=TRUE))

cp %>%group_by(reducer) %>% mutate(time=replace_na(time, 3600)) %>% summarize(mean=gmean(time))
static %>%group_by(reducer) %>% mutate(time=replace_na(time, 3600)) %>% summarize(mean=gmean(time))
perturb %>%group_by(reducer) %>% mutate(time=replace_na(time, 3600)) %>% summarize(mean=gmean(time))

cp %>% group_by(reducer) %>% summarize(mean=gmean(polynomials, na.rm=TRUE))
static %>% group_by(reducer) %>% summarize(mean=gmean(polynomials, na.rm=TRUE))
perturb %>% group_by(reducer) %>% summarize(mean=gmean(polynomials, na.rm=TRUE))

cp %>% group_by(reducer) %>% summarize(mean=gmean(monomials, na.rm=TRUE))
static %>% group_by(reducer) %>% summarize(mean=gmean(monomials, na.rm=TRUE))
perturb %>% group_by(reducer) %>% summarize(mean=gmean(monomials, na.rm=TRUE))

cp %>% group_by(reducer) %>% summarize(mean=gmean(degree, na.rm=TRUE))
static %>% group_by(reducer) %>% summarize(mean=gmean(degree, na.rm=TRUE))
perturb %>% group_by(reducer) %>% summarize(mean=gmean(degree, na.rm=TRUE))

cp %>% group_by(reducer) %>% summarize(mean=gmean(sreductions, na.rm=TRUE))
static %>% group_by(reducer) %>% summarize(mean=gmean(sreductions, na.rm=TRUE))
perturb %>% group_by(reducer) %>% summarize(mean=gmean(sreductions, na.rm=TRUE))
```


```{r}
caption <- "Geometric means of the experiments over all instances (ignoring timeouts)."
t <- by_inst_alg %>% 
  group_by(algorithm, reducer) %>% 
  summarize(
    time=gmean(time, na.rm=TRUE),
    polynomials=gmean(polynomials, na.rm=TRUE),
    monomials=gmean(monomials, na.rm=TRUE),
    degree=gmean(degree, na.rm=TRUE),
    sreductions=gmean(sreductions, na.rm=TRUE)
  ) %>%
    rename(
      "Algorithm" = algorithm,
      "Reducer" = reducer,
      "$t$" = time,
      "$|G|$" = polynomials,
      "$|\\Supp(G)|$" = monomials,
      "$\\deg$" = degree,
      "sred" = sreductions
    )
  print(xtable(t,caption=caption), 
        include.rownames=F, 
        sanitize.text.function=function(x){x},
        NA.string = "NA")
```